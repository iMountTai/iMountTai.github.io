<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>ceshi</title>
      <link href="/ceshi.html"/>
      <url>/ceshi.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>FULL PARAMETER FINE-TUNING FOR LARGE LANGUAGE MODELS WITH LIMITED RESOURCES</title>
      <link href="/lomo.html"/>
      <url>/lomo.html</url>
      
        <content type="html"><![CDATA[<p>论文<a href="https://arxiv.org/abs/2306.09782">FULL PARAMETER FINE-TUNING FOR LARGE LANGUAGE MODELS WITH LIMITED RESOURCES</a></p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>该论文主要集中于低资源下大模型的训练问题，提出LOMO优化器，使得大模型全参数微调成本更低，单卡24G即能微调LLaMA7B。</p><h1 id="方法介绍"><a href="#方法介绍" class="headerlink" title="方法介绍"></a>方法介绍</h1><p><img src="/fig1.jpg" alt></p><p>伪代码如下<br><img src="/fig2.jpg" alt></p><p>借鉴了SGD优化器的使用，没有使用动量计算梯度，相对于SGD的区别在于按步更新模型参数，即计算某个参数的梯度后立即更新该参数的梯度，而不是传统优化器那样计算完梯度后全量更新模型参数。使得优化器的状态不会被存储在显存中，且只需要存储一个参数的梯度，显著降低了使用的显存。</p><p>显存降低情况如下图所示<br><img src="/table1.jpg" alt></p><p>不过该方法的训练速度比较慢，可能是使用了deepspeed zero3以及按步更新模型参数的原因。实测该速度还不如int4 lora微调llama，不过这是全量微调，且使用的显存明显降低，算是提供了一种可行性方式。<br><img src="/table2.jpg" alt></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img src="/table3.jpg" alt></p><p>该实验结果都是基于自己的实验上比较的，未和公开的实验论文相比较，且实验结果是按照最好结果报告的，具有一定的参考意义，不过证明了LOMO的可行性。</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Pre-training </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>QLoRA-Efficient Finetuning of Quantized LLMs</title>
      <link href="/qlora.html"/>
      <url>/qlora.html</url>
      
        <content type="html"><![CDATA[<p>论文<a href="https://arxiv.org/abs/2305.14314">QLORA: Efficient Finetuning of Quantized LLMs</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>作者提出了QLORA，一种高效的微调方法，可以在单个48GB的GPU上微调一个65B参数的模型，同时保持完整的16位微调任务性能，从而降低内存使用。QLORA通过将梯度反向传播到一个冻结的、4位量化的预训练语言模型中，然后传播到LoRA。并将最好的模型系列命名为Guanaco，在Vicuna基准测试中表现优于所有先前公开发布的模型，仅需要在单个GPU上进行24小时的微调即可达到ChatGPT性能水平的99.3%。QLORA引入了许多创新来节省内存，而不牺牲性能：（a）4位NormalFloat（NF4），这是一种对于正态分布权重在信息理论上最优的新数据类型；（b）双重量化，通过量化量化常数来减少平均内存占用；（c）分页优化器，用于管理内存峰值。作者使用QLORA微调了1000多个模型，并对8个指令数据集、多个模型类型（LLaMA、T5）以及通常无法使用常规微调运行的模型规模（例如33B和65B参数模型）进行了详细的指令遵循和聊天机器人性能分析。结果显示，QLoRA在一个小的高质量数据集上微调可以实现最先进的结果，甚至在使用比先前最先进模型更小的模型时也是如此。文章基于人类和GPT-4评估提供了详细的聊天机器人性能分析，表明GPT-4评估是人类评估的一种廉价合理的替代方法。此外，作者还发现当前的聊天机器人基准测试不能够可靠地评估聊天机器人的性能水平。</p><h1 id="Training-setting"><a href="#Training-setting" class="headerlink" title="Training setting"></a>Training setting</h1><p>设置LoRA的r为64，α为16，并在基础模型的所有线性层上添加LoRA模块。对于13B及以下的模型，使用Adam的beta2为0.999，最大梯度范数为0.3，以及LoRA的dropout为0.1；而对于33B和65B的模型，dropout设为0.05。<br><img src="/setting.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Pre-training </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>deepspeed</title>
      <link href="/deepspeed.html"/>
      <url>/deepspeed.html</url>
      
        <content type="html"><![CDATA[<p>deepspeed config 键值使用</p><ul><li>train_batch_size：设置训练时的批量大小。</li><li>gradient_accumulation_steps：设置梯度累积的步数，以减少通信开销和内存占用。</li><li>fp16：设置是否使用混合精度训练，以提高速度和减少内存占用。</li><li>zero_optimization：设置是否使用ZeRO优化器，以进一步减少内存占用和提高扩展性。</li><li>optimizer：设置训练优化器的类型和参数，如Adam、Lamb等。</li><li>scheduler：设置学习率调度器的类型和参数，如WarmupLR、OneCycle等。</li><li>train_micro_batch_size_per_gpu：设置每个GPU上训练时的微批量大小。</li><li>steps_per_print：设置每隔多少步打印训练日志。</li><li>wall_clock_breakdown：设置是否记录训练过程中各个部分的时间消耗。</li><li>communication_data_type：设置通信时使用的数据类型，如float16、float32等。</li><li>sparse_gradients：设置是否使用稀疏梯度，以减少通信开销。</li><li>gradient_clipping：设置是否使用梯度裁剪，以防止梯度爆炸。</li><li>prescale_gradients：设置是否在累积梯度之前进行缩放，以防止数值溢出</li></ul>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> chatgpt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MobileBERT a Compact Task-Agnostic BERT for Resource-Limited Devices</title>
      <link href="/mobilebert.html"/>
      <url>/mobilebert.html</url>
      
        <content type="html"><![CDATA[<p>论文<br><a href="https://arxiv.org/pdf/2004.02984.pdf">MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices</a></p><h1 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h1><p>MobileBERT被设计成和$BERT_{large}$一样深，而每一层都通过采用bottleneck结构和平衡注意力机制和前馈网络进而使其变得更加狭窄（下图）。为了训练MobileBERT，我们首先训练了一个特殊设计的教师模型，一个包含$BERT_{large}$模型（IB-BERT）的inverted-bottleneck模型。然后，我们将知识从IB-BERT迁移到MobileBERT。我们在实证研究中仔细研究了各种知识迁移策略。<br><img src="/f1.jpg" alt><br>实证评估表明，MobileBERT比BERT（BASE）小4.3倍、快5.5倍，并且可以在知名的NLP基准上获得可观的结果。在GLUE的自然语言推理任务中，MobileBERT的GLUE得分为77.7，仅比BERT（BASE）低0.6，在Pixel 4手机上的延迟为62ms。在SQuAD v1.1/v2.0问答任务中，MobileBER获得的dev F1得分为90.3/80.2，甚至比BERT（BASE）高出1.5/2.1。</p><h1 id="MobileBERT"><a href="#MobileBERT" class="headerlink" title="MobileBERT"></a>MobileBERT</h1><p><img src="/t1.jpg" alt></p><h2 id="瓶颈层和倒瓶颈层"><a href="#瓶颈层和倒瓶颈层" class="headerlink" title="瓶颈层和倒瓶颈层"></a>瓶颈层和倒瓶颈层</h2><p>MobileBERT的体系结构如图1（c）所示。它和$BERT_{large}$一样深，但每个构建块都小得多。如表1所示，每个构建块的隐状态大小仅为128。另一方面，我们为每个构建块引入两个线性变换，以将其输入和输出维度调整为512。根据（He等人，2016）中的术语，我们将这种架构称为瓶颈层。</p><p>训练这样一个深并且瘦小的网络是很大的挑战。为了解决这一问题，我们首先构建了一个教师网络，并对其进行训练直至收敛，然后将知识从这个教师网络迁移到MobileBERT。我们发现这比从头开始直接训练MobileBERT的效果要好得多。各种训练策略将在后面的章节中讨论。这里，我们介绍教师网络的架构设计，如图1（b）所示。事实上，教师网络就是$BERT_{large}$，同时增加了倒瓶颈结构（Sandler等人，2018），将其feature map大小调整为512。在下文中，我们将教师网络称为IB-BERT（LARGE）。请注意，IB-BERT和MobileBERT具有相同的feature map大小，即512。因此，我们可以直接比较IB-BERT和MobileBERT的layer-wise输出的差异。我们的知识迁移战略需要这样一种直接的比较。</p><p>值得指出的是，同时引入瓶颈和倒瓶颈结构使得体系结构设计具有相当的灵活性。我们可以只使用MobileBERT的瓶颈（相应地，教师变为BERT）或者只使用IB-BERT的倒瓶颈（MobileBERT中没有瓶颈）来对齐它们的特征映射。然而，当使用这两种方法时，我们可以允许IB-BERT（LARGE）在保证MobileBERT足够紧凑的同时保持$BERT_{large}$的性能。</p><h2 id="堆叠式前馈网络"><a href="#堆叠式前馈网络" class="headerlink" title="堆叠式前馈网络"></a>堆叠式前馈网络</h2><p>MobileBERT的瓶颈结构带来的一个问题是，多头注意力（MHA）模块和前馈网络（FFN）模块之间的平衡被打破。MHA和FFN在Transformer结构中扮演着不同的角色：前者允许模型共同处理来自不同子空间的信息，而后者增加了模型的非线性。在原始的BERT中，MHA和FFN中的参数之比总是1:2。但在瓶颈结构中，MHA的输入来自更宽的特征映射（块间大小），而FFN的输入来自更窄的瓶颈（块内大小）。这导致MobileBERT中的MHA模块相对包含更多的参数。</p><p>为了解决这个问题，我们建议在MobileBERT中使用堆叠式前馈网络来重新平衡MHA和FFN之间的相对大小。如图1（c）所示，每个MobileBERT层包含一个MHA，但多个堆叠的FFN。在MobileBERT中，我们在每个MHA之后使用4个堆叠FFN。</p><h1 id="操作优化"><a href="#操作优化" class="headerlink" title="操作优化"></a>操作优化</h1><p><strong>移除层归一化</strong> 将n-channel的隐状态h的层归一化替换为element-wise的线性变换：<br>$\text{NoNORm(h)} = \gamma \circ h+β$<br><strong>使用relu激活</strong> 我们用更简单的relu激活代替了gelu激活（Nair和Hinton，2010）。</p><h2 id="Embedding因子分解"><a href="#Embedding因子分解" class="headerlink" title="Embedding因子分解"></a>Embedding因子分解</h2><p>BERT模型中的嵌入表占模型大小的很大一部分。为了压缩嵌入层，如表1所示，我们将MobileBERT中的嵌入维度减少到128。然后，我们对原始token嵌入应用核大小为3的一维卷积来产生512维的输出。</p><p>训练目标</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识蒸馏 </tag>
            
            <tag> BERT </tag>
            
            <tag> KD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</title>
      <link href="/bert-to-lstm.html"/>
      <url>/bert-to-lstm.html</url>
      
        <content type="html"><![CDATA[<p>论文<br><a href="https://arxiv.org/pdf/1903.12136.pdf">Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</a></p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h1><p><strong>模型压缩</strong> 一项突出的工作致力于压缩大型神经网络以加速推理.早期的开创性工作包括LeCun等人(1990)，他们提出了一种基于局部误差的方法来修剪不重要的权重。最近，Han等人(2015)提出了一种简单的压缩pipeline，在不损害精度的情况下，实现了模型尺寸的40倍缩小。不幸的是，这些技术导致不规则的权重稀疏，这排除了高度优化的计算例程。</p><h1 id="3-文章的工作"><a href="#3-文章的工作" class="headerlink" title="3 文章的工作"></a>3 文章的工作</h1><h2 id="3-1-模型结构"><a href="#3-1-模型结构" class="headerlink" title="3.1 模型结构"></a>3.1 模型结构</h2><p>教师模型采用预训练微调的BERT，对于输入的句子对，通过BERT计算一个特征向量$h$,在$h$的基础上构建任务分类器。对于单一句子分类，直接构建softmax层，预测概率为$y^{B}=softmax(Wh)$,$W \in \Bbb{R}^{k \times d}$是softmax的权重矩阵,$k$为标签数量。对于句子对任务，拼接两个句子的BERT特征向量送进softmax层。在训练过程中，我们利用交叉熵损失最大化正确标签的概率，联合微调BERT和分类器的参数</p><p>相比之下，我们的学生模型是单层的具有非线性分类器的BiLSTM。将输入词嵌入输入BiLSTM后，将每个方向上最后一步的隐藏状态拼接，输入带有ReLUs的全连接层，该层的输出再传递到softmax层进行分类.</p><img src="/bert-to-lstm/figure1.jpg" class><p>对于句子对任务，在两个句子编码（$h_{s1}$和$h_{s2}$）的连体结构中共享BiLSTM的编码器，如下图所示</p><img src="/bert-to-lstm/f2.jpg" class><p>然后对两个句子向量应用标准的concatenate–compare操作$f(h_{s1},h_{s2})=[h_{s1},h_{s2},h_{s1} \odot h_{s2},|h_{s1}-h_{s2}|]$，$\odot$表示按元素乘法。然后把该输出送到ReLU激活的分类器中。</p><p>应该强调的是，为了重新审视BiLSTM本身的表示能力，约束了工程实现。我们避免了任何额外的技巧，比如注意和层归一化。</p><h2 id="3-2-蒸馏目的"><a href="#3-2-蒸馏目的" class="headerlink" title="3.2 蒸馏目的"></a>3.2 蒸馏目的</h2><p>给定数据，学生模型模仿教师模型的输出。除了最后的标签，教师模型的预测概率同样重要。在二分类情感分析中，一些句子具有较强的情感属性，一些句子则是中性的。如果只用最后的预测标签训练学生模型，会丢失不确定性预测的有价值信息。softmax的参数认为是logits。学生模型在logits上训练使得训练更容易，因为教师模型在所有目标上学到的关系是同等重要的。</p><p>使用MSE损失实现$\mathcal{L}_{distill} = ||z^{(B)}-z^{(S)}||_2^2$，$z^{(B)}$和$z^{(S)}$分别是教师模型和学生的logits.交叉熵损失其实也行，但是本文实验证明MSE更好一点。</p><p>训练损失：<br>$\mathcal{L}=\alpha \cdot \mathcal{L}_{CE}+(1-\alpha) \cdot \mathcal{L}_{distill} = -\alpha \sum_{i} t_i \log {y_i^{(S)}} - (1 - \alpha)||z^{(B)}-z^{(S)}||_2^2$<br>其中$t$是ground_truth_label。对于没有标签的数据，预测label使用教师模型预测的label，即$t_i=1$如果$i=\operatorname{argmax} {y^{(B)}}$否则为0</p><h2 id="3-3-数据增强"><a href="#3-3-数据增强" class="headerlink" title="3.3 数据增强"></a>3.3 数据增强</h2><p>利用教师提供的伪标签（pseudo-labels），用一个大的、未标记的数据集来增强训练集，以帮助有效地提取知识。具体来说，随机执行以下操作：<br><strong>masking</strong>：给定一个概率$p_{mask}$，随机用toekn <em>[MASK]</em> 代替,对应于我们的模型中的未知标记和BERT中的掩码字标记.文章认为该工作能阐明每个词对标签的贡献。<br><strong>POS-guided word replacement</strong>，给定一个概率$p_{post}$,将该单词替换为相同词性（POS）的标签。为了保证原始训练分布，新单词从由词性 (POS) 标签重新归一化的一元词分布中采样。例如将 “What do pigs eat” 替换成 “How do pigs eat”。<br><strong>n-gram sampling</strong>：给定一个概率$p_{ng}$,在 1-5 中随机采样得到 n，然后采用 n-gram，去掉其他单词。这个规则在概念上等同于去掉示例中的所有其他单词，这是一种更激进的屏蔽形式。<br>流程如下：给定一个训练例子{$w_1,…w_n$},迭代单词$w_i$制uniform distribution,如果$x_i&lt;p_{mask}$，对$w_i$进行masking操作，如果$p_{mask} \le x_i&lt;p_{mask}+p_{pos}$，则进行POS-guided word replacement，这两种操作是相互排斥的，即一旦应用了一条规则，另一条就会被忽略。遍历单词之后，在整个合成的例子中采用n-gram采样。这个最后合成的例子加入到增强的无监督数据集中。<br>对单一样本进行n次迭代处理生成n个采样结果。对于句子对任务，我们通过只增强第一个句子(保持第二个固定值)、只增强第二个句子(保持第一个固定值)和两个句子进行循环。</p><h1 id="4-实验设置"><a href="#4-实验设置" class="headerlink" title="4 实验设置"></a>4 实验设置</h1><p>使用$BERT_{LARGE}$作为教师模型，加载预训练权重并遵循原始、特定任务的微调。文章微调是，选择用Adam优化器并设置学习率为{2，3，4，5}*$10^{-5}$,然后选择验证集上效果最好的模型。<br>学生模型，将原始数据及合成数据送给教师模型预测获得预测标签（soft logit targets），用以蒸馏$BiLSTM_{SOFT}$，即设置$\alpha = 0$，这种效果是最好的。</p><h2 id="4-1-数据集"><a href="#4-1-数据集" class="headerlink" title="4.1 数据集"></a>4.1 数据集</h2><p>SST-2,MNLI,QQP</p><h2 id="4-2-超参"><a href="#4-2-超参" class="headerlink" title="4.2 超参"></a>4.2 超参</h2><p>看论文吧</p><h1 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h1><p>效果比不上大模型，但是比原始的LSTM好，相对大模型就是参数少，响应快。文章认为较浅的bilstm对自然语言任务的表达能力比之前认为的更强</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识蒸馏 </tag>
            
            <tag> BERT </tag>
            
            <tag> KD </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
